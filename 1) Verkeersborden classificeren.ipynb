{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verkeersborden herkennen met Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![verkeersbord](images/4202.jpg)\n",
    "\n",
    "In deze workshop gaan we met deep learning verkeersborden herkennen. Het doel van dit deel is om kort uit te leggen wat deep learning is, en vervolgens een eerste model te bouwen en te evalueren. Dit project is geinspireerd door deze [pagina](https://medium.com/@waleedka/traffic-sign-recognition-with-tensorflow-629dffc391a6)\n",
    "\n",
    "## Neurale Netwerken\n",
    "Neurale Netwerken (NN's) zijn computer programma's die patronen kunnen herkennen. Ze zijn geinspireerd op hoe hersenen zijn ontworpen: Neuronen (Nodes) die aan elkaar verbonden zijn en leren hoe ze moeten reageren op inkomende signalen. Die nodes zijn georganiseerd in lagen.\n",
    "\n",
    "![Neuraal netwerk](images/tikz41.png)\n",
    "\n",
    "NN's bestaan al jaren, maar zijn tegenwoordig weer populair omdat de techniek toelaat dat we veel lagen gebruiken: Deep Learning.\n",
    "\n",
    "Dit notebook leert je een vrij ondiep netwerk te bouwen met behulp van de Python module Tensorflow. \n",
    "\n",
    "## Eerste poging: Verkeersborden classificeren\n",
    "Gegeven een afbeelding van een verkeersbord zullen we moeten bepalen wat voor verkeersbord we zien (bijvoorbeeld: Een voorrangsbord of een stopteken).\n",
    "\n",
    "Voor de project gebruiken we Python 3.5 en de libraries Tensorflow, Numpy, Sci-kit Image, and Matplotlib. Deze libraries zijn al in de environment die we net hebben aagemaakt ge√Ønstalleerd. Vervolgens gaan we de libraries importeren, dit doen  we met het commando 'import':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import os\n",
    "import random\n",
    "import skimage\n",
    "import skimage.data\n",
    "import skimage.transform\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imread\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Allow image embeding in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "print(str(\"inladen libraries is klaar!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dataset\n",
    "\n",
    "We gebruiken in dit project een verzameling verkeersborden uit [Belgie](http://btsd.ethz.ch/shareddata/). De beelden zijn opgedeeld in twee mappen:\n",
    "\n",
    "```\n",
    "/datasets/Training/\n",
    "/datasets/Testing/\n",
    "```\n",
    "Beide mappen hebben 62 sub-mappen (0000 tot 0061). De naam van elke sup-map representeert een type verkeersborden (het label). \n",
    "\n",
    "Als we de volgende twee cellen runnen, dan worden de afbeeldingen en labels (Training, Testing) gedownload onder de map datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://btsd.ethz.ch/shareddata/BelgiumTSC/BelgiumTSC_Training.zip\n",
    "\n",
    "zf = ZipFile(\"BelgiumTSC_Training.zip\")\n",
    "zf.extractall(\"./datasets/\")\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://btsd.ethz.ch/shareddata/BelgiumTSC/BelgiumTSC_Testing.zip\n",
    "    \n",
    "zf = ZipFile(\"BelgiumTSC_Testing.zip\")\n",
    "zf.extractall(\"./datasets/\")\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De traindata inladen\n",
    "\n",
    "Vervolgens gaan we data die we net gedownload hebben, inladen in Python met een functie def load_data die we hier aanmaken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    \"\"\"Loads a data set and returns two lists:\n",
    "    \n",
    "    images: a list of Numpy arrays, each representing an image.\n",
    "    labels: a list of numbers that represent the images labels.\n",
    "    \"\"\"\n",
    "    # Get all subdirectories of data_dir. Each represents a label.\n",
    "    directories = [d for d in os.listdir(data_dir) \n",
    "                   if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    # Loop through the label directories and collect the data in\n",
    "    # two lists, labels and images.\n",
    "    labels = []\n",
    "    images = []\n",
    "    for d in directories:\n",
    "        label_dir = os.path.join(data_dir, d)\n",
    "        file_names = [os.path.join(label_dir, f) \n",
    "                      for f in os.listdir(label_dir) if f.endswith(\".ppm\")]\n",
    "        # For each label, load it's images and add them to the images list.\n",
    "        # And add the label number (i.e. directory name) to the labels list.\n",
    "        for f in file_names:\n",
    "            images.append(imread(f))\n",
    "            labels.append(int(d))\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "# Load training and testing datasets.\n",
    "ROOT_PATH = \"./\"\n",
    "train_data_dir = os.path.join(ROOT_PATH, \"datasets/Training\")\n",
    "test_data_dir = os.path.join(ROOT_PATH, \"datasets/Testing\")\n",
    "\n",
    "images, labels = load_data(train_data_dir)\n",
    "\n",
    "print(\"Data is ingeladen in Python!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hebben nu twee lijsten:\n",
    "\n",
    "* **images** een lijst van plaatjes in de vorm van een numpy array.\n",
    "* **labels** een lijst met labels in de vom van een array van getallen van 0 tot en met 61\n",
    "\n",
    "De lijsten staan met elkaar in verbinding doo de gehanteerde volgorde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De data verkennen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We printen nu even hoeveel labels en afbeeldingen we hebben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique Labels: {0}\\nTotal Images: {1}\".format(len(set(labels)), len(images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder maken we een tabel met van elk label het eerste voorbeeld:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_and_labels(images, labels):\n",
    "    \"\"\"Display the first image of each label.\"\"\"\n",
    "    unique_labels = set(labels)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    i = 1\n",
    "    for label in unique_labels:\n",
    "        # Pick the first image for each label.\n",
    "        image = images[labels.index(label)]\n",
    "        plt.subplot(8, 8, i)  # A grid of 8 rows x 8 columns\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Label {0} ({1})\".format(label, labels.count(label)))\n",
    "        i += 1\n",
    "        _ = plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "display_images_and_labels(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kijk eens! Dat ziet er goed uit! Zo te zien hebben we van alle verkeersborden een plaatje dat min of meer het hele beeld vult. Het valt wel op dat de plaatjes niet helemaal vierkant zijn, dat moeten we later wel oplossen. \n",
    "\n",
    "Laten we eerst nog eens een type verkeersbord wat meer in detail bekijken. Laten we eens beginnen met verkeersbord type 42 (label 42)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_label_images(images, label):\n",
    "    \"\"\"Display images of a specific label.\"\"\"\n",
    "    limit = 24  # show a max of 24 images\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    i = 1\n",
    "\n",
    "    start = labels.index(label)\n",
    "    end = start + labels.count(label)\n",
    "    for image in images[start:end][:limit]:\n",
    "        plt.subplot(3, 8, i)  # 3 rows, 8 per row\n",
    "        plt.axis('off')\n",
    "        i += 1\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "display_label_images(images, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok we zien dat alle \"verboden parkeren borden\" in dezelfde klasse vallen! Het neuraal netwerk zou daar in principe mee om moeten kunnen gaan, maar het is wel goed om dit even gezien te hebben.\n",
    "\n",
    "Kan je nu zelf andere klasses bekijken? Probeer bijvoorbeeld eens type 26 en 27 te bekijken (door de 42 in 26 of 27 te veranderen). Wat zou dit straks kunnen betekenen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verschillende formaten\n",
    " \n",
    "Het is over het algemeen handig om de plaatjes in hetzelfde formaat te hebben. We hebben al gezien dat dat in dit geval niet zo is. Een goede manier om dit op te lossen is om de plaatjes handmatig bij te knippen zodat we geen belangrijke delen missen.\n",
    "\n",
    "Vandaag pakken we het snel aan, dus gaan we voor een hack: We schalen de plaatjes automatisch zonder te knippen. We gooien dus niks weg maar vervormen de beelden dus licht.\n",
    "\n",
    "Hoe groot zijn de plaatjes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in images[:5]:\n",
    "    print(\"shape: {0}, min: {1}, max: {2}\".format(image.shape, image.min(), image.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rond de 128 bij 128 dus. Als we ze nou eens allemaal afbeelden op 32x32, dan hebben we minder data (dus sneller resultaat) en ze zijn allemaal van hetzelfde formaat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Resize images\n",
    "images32 = [skimage.transform.resize(image, (32, 32))\n",
    "                for image in images]\n",
    "display_images_and_labels(images32, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minder scherp, maar nog steeds herkenbaar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum Viable Model\n",
    "Nu bouwen we een simpel neuraal netwerk dat plaatjes voor ons gaat classificeren. Het bestaat uit twee input tensors (de plaatjes en de labels), een tussenlaagje dat de plaatjes plat maakt, gevolgd door een fully connected layer die de voorspelling maakt.\n",
    "\n",
    "Een fully connected layer verbindt alle inputs naar elk van zijn eigen nodes. Deze layer heeft er 62 (een voor elke klasse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_a = np.array(labels)\n",
    "images_a = np.array(images32)\n",
    "print(\"labels: \", labels_a.shape, \"\\nimages: \", images_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph to hold the model.\n",
    "graph = tf.Graph()\n",
    "\n",
    "# Create model in the graph.\n",
    "with graph.as_default():\n",
    "    # Placeholders for inputs and labels.\n",
    "    images_ph = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    labels_ph = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    # Flatten input from: [None, height, width, channels]\n",
    "    # To: [None, height * width * channels] == [None, 3072]\n",
    "    images_flat = tf.contrib.layers.flatten(images_ph)\n",
    "\n",
    "    # Fully connected layer. \n",
    "    # Generates logits of size [None, 62]\n",
    "    logits = tf.contrib.layers.fully_connected(images_flat, 62, tf.nn.relu)\n",
    "\n",
    "    # Convert logits to label indexes (int).\n",
    "    # Shape [None], which is a 1D vector of length == batch_size.\n",
    "    predicted_labels = tf.argmax(logits, 1)\n",
    "\n",
    "    # Define the loss function. \n",
    "    # Cross-entropy is a good choice for classification.\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = labels_ph))\n",
    "\n",
    "    # Create training op.\n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "    # And, finally, an initialization op to execute before training.\n",
    "    # TODO: rename to tf.global_variables_initializer() on TF 0.12.\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "print(\"images_flat: \", images_flat)\n",
    "print(\"logits: \", logits)\n",
    "print(\"loss: \", loss)\n",
    "print(\"predicted_labels: \", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We gaan het model nu 200 keer trainen.(We gebruiken de ADAM optimizer om de cross-entropy zo laag mogelijk te krijgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session to run the graph we created.\n",
    "session = tf.Session(graph=graph)\n",
    "\n",
    "# First step is always to initialize all variables. \n",
    "# We don't care about the return value, though. It's None.\n",
    "_ = session.run([init])\n",
    "\n",
    "print(\"sessie is aangemaakt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tijdens het trainen verschijnt de parameter: loss. De loss zegt iets over hoe goed het model werkt. In het begin zal de loss nog erg hoog zijn, hoe meer er getraind is, hoe kleiner de loss. We streven naar een loss ver onder de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(201):\n",
    "    _, loss_value = session.run([train, loss], \n",
    "                                feed_dict={images_ph: images_a, labels_ph: labels_a})\n",
    "    if i % 10 == 0:\n",
    "        print(\"Loss: \", loss_value)\n",
    "        \n",
    "print(\"het model is getraind!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan je de loss nog lager maken? Denk bijv. aan vaker trainen (getal 201 groter maken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Het model toepassen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laten we eens kijken welk percentage ons model nu goed heeft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions against the full test set.\n",
    "predicted = session.run([predicted_labels], \n",
    "                        feed_dict={images_ph: images32})[0]\n",
    "# Calculate how many matches we got.\n",
    "match_count = sum([int(y == y_) for y, y_ in zip(labels, predicted)])\n",
    "accuracy = match_count / len(labels)\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Een percentage rond de 65% procent... klinkt goed!\n",
    "\n",
    "Hoe ziet dat er dan uit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 10 random images\n",
    "sample_indexes = random.sample(range(len(images32)), 10)\n",
    "sample_images = [images32[i] for i in sample_indexes]\n",
    "sample_labels = [labels[i] for i in sample_indexes]\n",
    "\n",
    "# Run the \"predicted_labels\" op.\n",
    "predicted = session.run([predicted_labels], \n",
    "                        feed_dict={images_ph: sample_images})[0]\n",
    "\n",
    "# Display the predictions and the ground truth visually.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(len(sample_images)):\n",
    "    truth = sample_labels[i]\n",
    "    prediction = predicted[i]\n",
    "    plt.subplot(5, 2,1+i)\n",
    "    plt.axis('off')\n",
    "    color='green' if truth == prediction else 'red'\n",
    "    plt.text(40, 10, \"Truth:        {0}\\nPrediction: {1}\".format(truth, prediction), \n",
    "             fontsize=12, color=color)\n",
    "    plt.imshow(sample_images[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kun je verklaren waarom het model sommige verkeersborden verkeerd heeft voorspeld?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluatie\n",
    "\n",
    "We scoren nu aardig op de trainset, maar heeft dit model nu ook echt iets geleerd?\n",
    "\n",
    "Daar hebben we de testset voor, weet je nog?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset.\n",
    "test_images, test_labels = load_data(test_data_dir)\n",
    "\n",
    "print(\"de testset is ingeladen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the images, just like we did with the training set.\n",
    "test_images32 = [skimage.transform.resize(image, (32, 32))\n",
    "                 for image in test_images]\n",
    "display_images_and_labels(test_images32, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions against the full test set.\n",
    "predicted = session.run([predicted_labels], \n",
    "                        feed_dict={images_ph: test_images32})[0]\n",
    "# Calculate how many matches we got.\n",
    "match_count = sum([int(y == y_) for y, y_ in zip(test_labels, predicted)])\n",
    "accuracy = match_count / len(test_labels)\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De score voor de testset is iets lager, weet je ook waardoor dat zou kunnen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the session. This will destroy the trained model.\n",
    "session.close()\n",
    "\n",
    "print(\"de sessie is weer gesloten!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lukt het om de score voor de trainset en testset hoger te krijgen? Denk aan vaker trainen, hogere resolutie beelden erin stoppen (dus geen 32x32 maar bijv. 64x64 of 128x128)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Deze workshop is gemaakt door RWS Datalab, laatste wijziging: 26-6-2020<i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
